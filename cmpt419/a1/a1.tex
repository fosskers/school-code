\documentclass{article}
% GENERAL
\usepackage{setspace,mathtools,amsfonts,amsmath,amsthm,amssymb,hyperref}
\usepackage{tikz,epigraph,pgfplots}  % For trees
\usepackage[utf8]{inputenc}
\usepackage{tikz-qtree,tikz-qtree-compat}
\usepackage{forest}

% FOR SOURCE CODE
\usepackage{listings}
% Default settings for code listings
\lstset{frame=tb,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  frame=single,
  breaklines=true,
  breakatwhitespace=true
  tabsize=4
}

% FOR TREE DRAWING
\pgfplotsset{compat=newest}
\usetikzlibrary{shapes.geometric,arrows,fit,matrix,positioning}
\tikzset
{
      treenode/.style = {circle, draw=black, align=center, minimum size=1cm}
}

% MARGINS
\usepackage[left=1in,top=1in,right=1in,bottom=1in]{geometry}
\onehalfspacing

\begin{document}
\title{CMPT 419 Assignment 1}
\author{Colin Woodbury\\ 301238755\\ cwoodbur@sfu.ca}
\date{\today}
\maketitle

% --- TABLE OF CONTENTS ---
\tableofcontents
\clearpage
% -------------------------

\section{Probabilistic Modeling}

\begin{enumerate}
\item Since there are multiple parties, but only one can win majority,
  we model this with a \fbox{Multinomial Distribution}. Election states
  are represented by some $x$, where $x_k \in \{0,1\}$ and $\sum_k x_k = 1$,
  meaning only one party can hold majority at once.\\
  Our $\mu_k$ values are the expections that a given party wins, such that
  $\sum_k \mu_k = 1$.
\item If each of the four parties had an equal chance of winning,
  we would see: $\mu = (0.25, 0.25, 0.25, 0.25)$
\item If the elections were rigged by the Conservatives, we would see:
  $\mu = (0,0,1,0)$.
\item \begin{align*}
  p(\mu | D) \propto p(D | \mu) \cdot p(\mu)
\end{align*}
  Where $\mu = (0,0,0,1)$, and $D$ is the dataset of polls showing NDP
  majority. Note $p(\mu) = 1$. $p(D | \mu)$ is the maximum likelihood
  function of the data.
\end{enumerate}

\section{Regularized Least-Squares Linear Regression}
Show that for $L_2$ least-squares regularization, $w = (\lambda I + \Phi^T\Phi)^{-1}\Phi^Tt$
\begin{proof}
  From a previous proof, we had that:
  \begin{align*}
    0^T = \mathbf{w}^T\Phi^T\Phi - \mathbf{t}^T\Phi
  \end{align*}
  Since regularized error takes the form:
  \begin{align*}
    E_D(\mathbf{w}) + \lambda E_W(\mathbf{w}) &=
    \frac{1}{2}\sum^N_{n=1}\{t_n - \mathbf{w}^T\phi(\mathbf{x_n})\}^2 + \frac{\lambda}{2}\mathbf{w}^T\mathbf{w}
  \end{align*}
  We introduce a regularized $\mathbf{w}$ term into the equation above:
  \begin{align*}
    0^T &= \mathbf{w}^T\Phi^T\Phi - \mathbf{t}^T\Phi + \lambda\mathbf{w}^T\\
    \mathbf{t}^T\Phi &= \lambda\mathbf{w}^T + \mathbf{w}^T\Phi^T\Phi\\
    \Phi^T\mathbf{t} &= \lambda\mathbf{w} + \Phi^T\Phi\mathbf{w}\\
    &= \lambda I\mathbf{w} + \Phi^T\Phi\mathbf{w}\\
    &= [\lambda I + \Phi^T\Phi]\mathbf{w}\\
    [\lambda I + \Phi^T\Phi]^{-1}\Phi^T\mathbf{t} &= \mathbf{w}
  \end{align*}
\end{proof}


\section{Training vs. Test Error}
\begin{enumerate}
\item \fbox{Yes.} If there are 9 data points or fewer, the two polynomials
will   both pass through the training points perfectly, and thus have zero  
training error.\\   If there are 10 points or more, the degree-10 polynomial
will have strictly   less training error than the degree-9.
\item \fbox{No.} If there are 9 data points or fewer, the degree-10 polynomial
  will curve more violently between each point, giving higher training
  error.\\
  If there are 10 points or more, the degree-10 polynomial will have
  strictly less error.
\item Always? \fbox{No.} One can imagine a scenario where a degree-1
  polynomial (a line) were fit to a higher degree curve from which the data
  truly comes. Training error would be high.
  Testing data could then by coincidence fit the line perfectly, although
  this is unlikely.
\end{enumerate}

\section{Regression}
\subsection{Getting Started}

\begin{enumerate}
\item Highest child mortality (U5MR) rate in 1990: Niger, with 313.7.
\item Highest child mortality (U5MR) rate in 2011: Sierra Leone, with 185.3.
\item The function detects \_ values in each row, and replaces
  them with the median of all other valid entries from that row.
\end{enumerate}

\subsection{Polynomial Regression}

\begin{enumerate}
\item \begin{center}
  \includegraphics[scale=0.9]{reg1}
\end{center}
  Notice that the training error increases with degree. We fix this
  by normalizing the data:
  \begin{center}
      \includegraphics[scale=0.9]{reg2}
  \end{center}
  Now, as expected, our training error decreases as polynomial degree
  increases. Test error increases, which demonstrates the issue
  of over-fitting.
\item \begin{center}
  \includegraphics[scale=0.9]{reg3}
\end{center}
  Testing error is irregularly high for Feature 11. By plotting
  the data with our fitted curves, we can gain intuition as to why:
  \begin{center}
    \includegraphics[scale=0.9]{plot11}
  \end{center}
  Below we provide plots for features 12 and 13 for comparison:
  \begin{center}
    \includegraphics[scale=0.9]{plot12}
    \includegraphics[scale=0.9]{plot13}
  \end{center}
  These two curves seem better behaved.
\end{enumerate}

\subsection{Sigmoid Basis Functions}

\begin{enumerate}
\item Use of a sigmoid basis function instead of a polynomial yields
  a nicer curve fit:
  \begin{center}
  \includegraphics[scale=0.9]{sig}
\end{center}
  The total training error is \fbox{28.41}, while the testing error is
  \fbox{32.11}.
\end{enumerate}

\subsection{Regularized Polynomial Regression}

\begin{enumerate}
\item
  \begin{center}
  \includegraphics[scale=0.9]{lambda}
\end{center}
  It's clear from the plot that \fbox{0.1} is the most appropriate lambda
  value. The error value for $\lambda = 0$ was \fbox{69.67}.
\end{enumerate}

\end{document}
