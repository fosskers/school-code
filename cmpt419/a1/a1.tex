\documentclass{article}
% GENERAL
\usepackage{setspace,mathtools,amsfonts,amsmath,amsthm,amssymb,hyperref}
\usepackage{tikz,epigraph,pgfplots}  % For trees
\usepackage[utf8]{inputenc}
\usepackage{tikz-qtree,tikz-qtree-compat}
\usepackage{forest}

% FOR SOURCE CODE
\usepackage{listings}
% Default settings for code listings
\lstset{frame=tb,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  frame=single,
  breaklines=true,
  breakatwhitespace=true
  tabsize=4
}

% FOR TREE DRAWING
\pgfplotsset{compat=newest}
\usetikzlibrary{shapes.geometric,arrows,fit,matrix,positioning}
\tikzset
{
      treenode/.style = {circle, draw=black, align=center, minimum size=1cm}
}

% MARGINS
\usepackage[left=1in,top=1in,right=1in,bottom=1in]{geometry}
\onehalfspacing

\begin{document}
\title{CMPT 419 Assignment 1}
\author{Colin Woodbury\\ 301238755\\ cwoodbur@sfu.ca}
\date{\today}
\maketitle

% --- TABLE OF CONTENTS ---
\tableofcontents
\clearpage
% -------------------------

\section{Probabilistic Modeling}

\begin{enumerate}
\item hi
\item hi
\item hi
\item hi
\end{enumerate}

\section{Regularized Least-Squares Linear Regression}

\section{Training vs. Test Error}
\begin{enumerate}
\item \fbox{Yes.} If there are 9 data points or fewer, the two polynomials
will   both pass through the training points perfectly, and thus have zero  
training error.\\   If there are 10 points or more, the degree-10 polynomial
will have strictly   less training error than the degree-9.
\item \fbox{No.} If there are 9 data points or fewer, the degree-10 polynomial
  will curve more violently between each point, giving higher training
  error.\\
  If there are 10 points or more, the degree-10 polynomial will have
  strictly less error.
\item Always? \fbox{No.} One can imagine a scenario where a degree-1
  polynomial (a line) were fit to a higher degree curve from which the data
  truly comes. Training error would be high.
  Testing data could then by coincidence fit the line perfectly, although
  this is unlikely.
\end{enumerate}

\section{Regression}
\subsection{Getting Started}

\begin{enumerate}
\item Highest child mortality (U5MR) rate in 1990: Niger, with 313.7.
\item Highest child mortality (U5MR) rate in 2011: Sierra Leone, with 185.3.
\item The function detects \_ values in each row, and replaces
  them with the median of all other valid entries from that row.
\end{enumerate}

\subsection{Polynomial Regression}

\begin{enumerate}
\item \begin{center}
  \includegraphics[scale=0.9]{reg2}
\end{center}
  Notice that the training error increases with degree. We fix this
  by normalizing the data:
  \begin{center}
      \includegraphics[scale=0.9]{reg1}
  \end{center}
  Now, as expected, our training error decreases as polynomial degree
  increases. Test error increases, which demonstrates the issue
  of over-fitting.
\item \begin{center}
  \includegraphics[scale=0.9]{reg3}
\end{center}
  Testing error is irregularly high for Feature 11. By plotting
  the data with our fitted curves, we can gain intuition as to why:
  \begin{center}
    \includegraphics[scale=0.9]{plot11}
    \includegraphics[scale=0.9]{plot12}
    \includegraphics[scale=0.9]{plot13}
  \end{center}
\end{enumerate}

\subsection{Sigmoid Basis Functions}

\begin{enumerate}
  \item hi
\end{enumerate}

\subsection{Regularized Polynomial Regression}

\begin{enumerate}
  \item hi
\end{enumerate}

\end{document}
